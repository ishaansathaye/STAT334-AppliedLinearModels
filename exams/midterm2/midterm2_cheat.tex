\documentclass[8pt]{extarticle}

\usepackage{color,fancyhdr,ifthen,amssymb,amsfonts,amsmath}
\usepackage{geometry}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{eso-pic}
\usepackage{blindtext}

\geometry{
    top=0.3in,
    bottom=0in,
    left=0.1in,
    right=0.1in,
}

\setlist{nolistsep} %\itemsep0em in itemize

\pagestyle{fancy}
\fancyhf{} % Clear header and footer
\renewcommand{\headrulewidth}{0pt} % Remove header rule
\renewcommand{\footrulewidth}{0pt} % Remove footer rule
\linespread{1.2} % Adjust the line spacing here
\setlength{\headsep}{0.05in} % Adjust the space between header and text
\setlength{\footskip}{0.1in} % Adjust the space between text and footer
\parindent=0in
\flushbottom % Ensure text is aligned with both top and bottom margins

% \newcommand{\head}[3]{\lhead{#1}\chead{#2}\rhead{\ifthenelse{\isodd{
%     \thepage}}{Ishaan Sathaye
%  {\hspace{.01in}}}{}}}

% \head{Midterm 1 Exam Cheat Sheet}{STAT 334}

\begin{document}

% Halfway line for Midterm 1:
% \AddToShipoutPictureBG{
% \AtTextCenter{\hspace{-0.6\textwidth}\rule{1.2\textwidth}{0.5pt}}}

\textit{\textbf{\underline{1:}}}
\textbf{Scatterplot:} form/shape, direction, strength (points follow 
recognizable form), unusual features.
\textbf{Association:} look at scales (might be misleading).
\textbf{PC Coeff:} strength of linear association; -1 to 1; 0 means no linear; r
does not change if variables are rescaled.
\textit{\textbf{\underline{2:}}}
\textbf{Residual:} observed - predicted; $e_i = y_i - \hat{y}_i$.
\textbf{Least Squares:} minimize SSE: $\sum_{i=1}^{n} e_i^2$.
\textbf{Interp Coeff:} slope coeff as the pred change in Y associated with a 
one-unit change in $X_i$, holding all other predictors constant; intercept as the 
predicted Y value when all preds are 0 (could be nonsensical).
\textbf{$R^2$:} coeff of determination; $ = \frac{SSE(\bar{y}) - SSE(\hat{y})}{SSE(\bar{y})}$;
unexplained variation in y / total var in y (look at points distribution).
\textbf{$R^2$ Interp:} percent reduction in SSE by taking into
account the predictors; percentage of variation in Y explained by the regression 
function with predictors; range is 0 to 1 and 1 if perfect predictions; $R^2 = r^2$
for simple linear regression.
\textit{\textbf{\underline{3:}}}
\textbf{Least Squares Estimate of $\beta$:} $SSE = \sum_{i=1}^{n} e^2_i = e^Te$;
$\hat{\beta} = (X^TX)^{-1}X^Ty$; $\hat{y} = X\hat{\beta} = Hy$.
\textbf{Hat Matrix:} $H = X(X^TX)^{-1}X^T$; symmetric and $h_{ij}$ describes 
the weight each of the values in the ith row of H have on the predicted value 
of $y_i$.
\textbf{Contributions:} $h_{ij}y_j$ is the actual contribution of j makes to the 
value $\hat{y_i}$.
\textit{\textbf{\underline{4:}}}
\textbf{MSE:} $s^2 = \frac{SSE}{n - p}$; $s$ or RMSE is the typical prediction 
error; expect 95\% of observed y values to lie roughly within $2s$ of predicted 
values; called \textbf{residual standard error in R}.
\textbf{s vs $R^2$:} both small s and large $R^2$ is the goal.
\textit{\textbf{\underline{5:}}}
\textbf{Perm. Test:} is there a relation between y and x find test stat that 
measures association for all possible perms of the resp var and compute the prop.
of times an observed test stat as extreme as the one from original sample; p-value 
from the graph by counting.
\textbf{p-value:} probability of obtaining a result (test stat) at least as extreme 
as the one observed, if the null were true; $<$ 0.1 is some, $<$ 0.05 is fairly 
strong, $<$ 0.01 is very strong, $<$ 0.001 is extremely strong; small p-value 
means result is unlikely to have occurred by chance alone, if the null were true 
making it statistically significant.
\textbf{Inference on $\beta_j$:} $t = \frac{\hat{\beta}_j - \beta_j}{SE(\hat{\beta}_j)}$;
$ 1 - \alpha$ confidence limits: $\hat{\beta}_j \pm t_{n-p, \alpha/2}SE(\hat{\beta}_j)$.
\textbf{Standard Error of $\beta$:} $SE(\hat{\beta}_j) = \sqrt{MSE(C_{j+1}C_{j+1})}$ 
where C is the jth column of $(X^TX)^{-1}$; \texttt{summary(fit)} gives null = 0
and alternative $\neq$ 0; \texttt{model.matrix(fit)} gives design matrix X.
\textbf{One Sided H:} \texttt{pt()} with lower tail false gives area to the right
(t $>$ 0) and true gives left tail area.
\textbf{68/95/99.7 Rule:} values like within 1/2/3 std dev of mean.
\textbf{t-value:} t $>$ 2 or $<$ -2 means results are significant.
\textbf{Critical Value:} \texttt{qt(p, df)} gives t value for area p to the left of it;
90/95/99 percent CI is p = 0.95/0.975/0.995.
\textbf{CI Slope Interp:} We are 95\% confident that the expected \textit{change} 
in the \textit{response} for each one \textit{unit} increase in the \textit{$X_i$}
falls between l and u, (after adjusting for other predictors in the model) 
\textbf{use only when interpreting about the effect of one predictor with multiple
predictors, say for conclusions too in inference tests}.
\textbf{Bonferroni:} $1 - \frac{1-C}{g}$ where C is the confidence level and g is
the number of coefficients being tested; want each individual confidence to be 
above C in order for the joint level to be above C; \textbf{only for intervals}.
\textbf{Joint CI Interp:} We are (at least) 95\% confident that all intervals 
correctly capture the population parameters.
\underline{6:}
\textbf{FINE Assumptions:} Form, Independence, Normality, Equal Variance;
Form: expect linear form, Independence: errors are independent (in data 
description), Normality: errors follow a normal dist, Equal Variance: variance 
of errors is the same.
\textbf{Plots:} Form: residuals vs fitted (no trend/curve) or residuals vs each 
$X_i$ \textbf{for multiple predictors}; Equal Variance: residuals vs fitted 
(no fan shape); Normality: qq plot (no big departure from straight line) or 
histogram (no big skew); Independence: look at residuals vs observations 
number (want to be random).
\textbf{Formal tests:} wilks $\Rrightarrow$ normality, pagan $\Rrightarrow$ equal 
variance, low p means violated.
\underline{7:}
\textbf{Interp of Slope:} only log transforms can be restated in terms of the 
original vars, preds can always be restated in terms of original vars.
\textbf{Transforming} Y: non-linearity, non-constant variance, and non-normality; 
X: non-linearity, high leverage, influence;
\textbf{Ladder of Powers:} p = 2, 1, 0.5, 0, -0.5, -1, -2; y* = $y^2, y,
\sqrt{y}, 1/\sqrt{y}, 1/y, 1/y^2$; right to become better; log and sqrt not
defined for zero or negative values, so transform $y/x + c$, where c makes all 
values $\geq 1$.
\textbf{Strategies:} skewed residuals: right skew is y down, left skew is y up;
residual var inc. as x incr: y down, decr as x incr: y up; non-linear: correct 
non-normal and unequal var then y, only non-linear then x.
\textbf{Non-linearity Bulges Point:} up and left is y up or x down; up and 
right is y up or x up; down and left is y down or x down; down and right is y 
down or x up.
\textbf{Interp of X Transform:} If we multiply $x_i$ by b (chosen log base) we 
predict a \textit{change} of $\hat{\beta_i}$ in the mean value of y after adjusting 
for the other vars in model.
\textbf{Interp of Y Transform:} Each one unit \textit{change} in $x_i$ \textit{changes} 
the 
predicted median value of y by a factor of $b^{\hat{\beta_i}}$ after \dots.
\textbf{Interp of Both:} A c-fold \textit{change} in $x_i$ \textit{changes} the 
predicted median 
value of y by a factor of $c^{\hat{\beta_i}}$ after \dots.
\textbf{Non-log Transform:} Each increase of one in $1/x_i$ is associated with 
an increase of $\hat{\beta_i}$ in predicted $\sqrt{y}$.
\textbf{Median:} median instead of mean because $E[log(Y)] \neq log[E(Y)]$ but 
for median it is true.
\textbf{Box-Cox:} round lambda to nearest 0.5.
\textbf{Matrix Scatterplot:} don't reflect preds act jointly.\\

\textit{\textbf{\underline{8A:}}}
\textbf{R-sq equation:} SSTO = SSR + SSE; $R^2 = \frac{SSR}{SSTO}$; variation 
explained by model / total variation in response; Adding vars moves SSE to SSR.
\textbf{Type 1 SS:} \texttt{anova(fit)}; order matters, after adjusting for vars 
earlier in model; additional var in y explained by adding x to model alreadly 
containing other x's.
\textbf{Type 2 SS:} \texttt{anova(fit, type = 'II')}; after adjusting for all 
other vars in model; output row is var being entered last.
\textit{\textbf{\underline{8B:}}}
\textbf{DF Type 1:} total = n-1, error = n-p, reg = p-1; MS = SS / df,
MS = SSR / k, MSE = SSE / (n-k-1).
\textbf{R-sq:} Adding additional vars can never decrease $R^2$; $R^2_{adj}$ 
$ = 1 - \frac{SSE/(n-k-1)}{SSTO/(n-1)} = 1 - \frac{n-1}{n-k-1}(1-R^2)$; can 
decrease when new unnecessary var is added to model.
\textbf{General F Test Props:} p-values found in right tail (upper tail), df = 
F(r, n-k-1), where r is number of vars in full model; FINE assumptions met for 
partial F test.
\textbf{Partial F-Test:} \texttt{anova(fit.reduced, fit.full)}; reduced is model
without variables; $H_0$: r of $\beta_j$ is 0, $H_a$: at least one $\beta_j \neq 
0$; F = $\frac{(SSE_{reduced} - SSE_{full})/r}
{MSE(full)}$; can replace SSE with SSR but full is first; numerator is reduction 
in SSE (boost in SSR) for full compared to reduced, dividing by r accounts for 
change in model complexity, standardize by MSE of full model.
\textbf{Model Utility Test:} \texttt{summary(fit)}; $H_0$: all $\beta_j$ are 0,
$H_a$: at least one $\beta_j \neq 0$; F = $\frac{(SSTO - SSE(full))
/k}{MSE(full)} = \frac{MSR}{MSE}$; numerator is the SSR(full); reduced model is 
mean model (no predictors);
\textbf{Single Coef Test:} \texttt{summary(fit) or Anova(fit, type='II')}; 
$H_0$: $\beta_j = 0$, $H_a$: $\beta_j \neq 0$; F = 
$\frac{(SSR(full) - SSR(all-but-x_j))/1}{MSE(full)}$; reduced model is full 
model without $x_j$; \textbf{Equivalent to t-test with $t^2$ = F and 
p-values are the same}.
\textbf{SS Total: Type 1 ANOVA table contains SST, not Type 2 b/c not 
sequential}.
\textbf{MUT Decision:} reject $H_0$ means we have sufficient evidence to conclude 
that at least one of vars is not equal to 0. Can conclude that at least one of 
vars is significantly useful in predicting y.
\textbf{SCT Decision:} large p-val means we do not have enough evidence to 
believe that var is any different form zero after adjusting for other vars, 
that is var does not significantly improve the model containing other vars.
\textbf{PFT Decision:} Fail to reject the null, adding r vars to model that already 
contains other vars does not significantly improve model. Not enough evidence 
that full containing all vars is better than reduced model, so keep reduced 
b/c of parsimony; Reject null, means larger has significant reduction in SSE, 
bost in SSR and $R^2$, and improves pred of Y.
\textbf{Dropping Vars:} cannot just drop all variables if not significant, need 
to come out one at a time or perform partial F test.
\textit{\textbf{\underline{10:}}}
\textbf{Point Estimate:} $\hat{y}_0 = x^T_0\hat{\beta}$; in R, \texttt{predict()}.
\textbf{Prediction Interval:} $t_{n-k-1, \alpha/2} s \sqrt{1 + x^T_0
(X^TX)^{-1}x_0}$; predicting the future response $y_0$ of an individual for a 
particular value of $x_0$, use "individual" in interp; wider than CI b/c of 
greater uncertainty around predicting individual observation; wider as we move 
further away from original combination of data; refer to as a chance since it 
is a future observation.
\textbf{Confidence Interval:} same but without the 1 + term; \textbf{predicting 
mean response for a particular value of $x_0$}; use "all" keyword.
\textit{\textbf{\underline{11:}}}
\textbf{Signs of MultiCol:} corr matrix values close to 1 or -1; coeffs do not 
expected signs; overall F test significant but no t-tests significant; large 
VIFs.
\textbf{VIF:} $VIF_j = \frac{1}{1 - R^2_{i}}$, where $R^2_{i}$ is the proportion 
of variation in $x_i$ that is explained by its linear relationship to other vars; 
measures increase in variability of ith sample reg coeff due to linear assoc of 
$x_i$ with other preds in model; $VIF_j = 1$ if not linearly related, SD is 
unchanged when other preds enter model; 5 or 10 means problems in estimation, 
4 means SD doubles, 10 means SD triples; Ex. regressing x on other x's;
\textbf{Added Variable Plot:} $residual(y\sim x_1+x_2)$ vs $residual(x_3\sim 
x_1+x_2)$ where first is y with the effect of x1 and x2 removed and second is x3 
with the effect of x1 and x2 removed; \textbf{whether $x_i$ has anything new 
to tell us (unique contribution) after removing effect of other vars}; intercept 
should always be 0 since SSE = 0; reg line goes through average which is 0; 
linear relationship means useful; ex. association between left over (unexplained) 
variation in y and x1 after adding x2 to the model (looking at x1); 
\textbf{Slope of LS Line:} what it means to adjust for other vars; slope of LS 
line in plot is the reg coeff of xi in multiple reg model; entire row is same in
output; large p-val means xi not useful to add to model that already has other
vars.
\textit{\textbf{\underline{12:}}}
\textbf{Leverage:} outlying in x-space; ith diagonal of hat matrix is leverage
of ith obs; $h_{ii}$ is leverage of ith obs; 0 to 1, $h_i > 2\bar{h} = 2(k+1)/n$
are leverage points.
\textbf{Influential:} removing obs changes fit of model; outlying in x and y not
consistent; large residuals mean outlying in y.
\textbf{Internally Studentized Residual:} $r_i = e_i / s\sqrt{1 - h_{i}}$; $r_i$
$>$ 3 is large residual; \texttt{rstandard(fit)}.
\textbf{Externally Studentized Residual:} $t_i = e_i / s_{(i)}\sqrt{1 - h_{i}}$;
$s_{(i)}$ is s when ith obs removed; $t_i > Bonferroni t^*$ is extreme; 
\texttt{rstudent(fit)}.
\textbf{DFFITS:} $\frac{\hat{y}_i - \hat{y}_{i(i)}}{s\sqrt{h_{i}}}$; comparing 
fitted values w/wo obs in data set and standardize by denom.
\textbf{Cook's Distance:} $D_i = \frac{1}{p+1}\frac{e_i^2}{s^2}
\frac{h_{ii}}{1 - h_{ii}}$ which is constant*internally studentized residual*
leverage; it is an aggregated measure of the effect of removing ith obs on all
predicted values.
\textbf{Guidelines Both:} $>$ 1 as being influential for n $<$ 30, greater than 
$2\sqrt{(k+1)/n}$ for large, $D_i$ greater than 0.5 or 1 as influential.\\
\\
% \textit{Midterm 2 Notes}
% \begin{itemize}
%     \item hard question is about $R^2$ because given a bunch of output and need 
%     to know where to look; handout 8 anova tables
%     \item 2 of the 3 F tests are on the Midterm
%     \item Nothing about handout 12 really on the midterm
%     \item No VIF problems but on that handout talked about added variable plots
%     which is important
%     \item transforming is back on the midterm (interpretations)
%     \item more plots that we have seen before (harder plots) residuals vs fitted
%     can have more than 1 assumption can be met
%     \item stuff on the sequential sum of squres (handout 8) - focus on this
% \end{itemize}

% \textit{Midterm 2 Notes: Krish's Class}
% \begin{itemize}
%     \item lots of tables
%     \item lots of stuff from Handout 8
%     \item stuff on multicollinearity, nothing about cook's distance, not much on
%     handout 12, nothing about leverage
%     \item lots of questions from HW 4, on anova type 1 and type 2, tables, 
%     sequential, etc.
%     \item hard question about Anova tables
%     \item know the SS for Anova and know true differnce between type 1 and type 
%     2 and whento use each specifically
%     \item one question from H10 about the intervals and R output (detailed 
%     questions about the output), know interpretation of this too
%     \item question on transformation similar to previous Midterm
%     \item hard question on $R^2$ specifically
% \end{itemize}

% \textit{Midterm Notes}
% \begin{itemize}
%     \item 12 open ended questions and 10 MC
%     \item issue with linearity with residuals vs fitted by looking at the form
%     \item 2 matrix problems (hat matrix)
%     \item 5 questions on the assumptions
%     \item transformation question is the hardest (worth 6 points)
%     \item after adjusting \dots when multiple predictors
%     \item given $R^2$ figure out s (typical prediction error) if hidden 
%     then find the numbers to calculate each thing
%     \item hard question on the bonferoni
%     \item know how to calculate a p-value (shade the area) (permutation test handout)
% \end{itemize}

\end{document}