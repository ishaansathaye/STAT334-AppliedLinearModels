\documentclass[8pt]{extarticle}

\usepackage{color,fancyhdr,ifthen,amssymb,amsfonts,amsmath}
\usepackage{geometry}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{eso-pic}
\usepackage{blindtext}

\geometry{
    top=0.3in,
    bottom=0in,
    left=0.1in,
    right=0.1in,
}

\setlist{nolistsep} %\itemsep0em in itemize

\pagestyle{fancy}
\fancyhf{} % Clear header and footer
\renewcommand{\headrulewidth}{0pt} % Remove header rule
\renewcommand{\footrulewidth}{0pt} % Remove footer rule
\linespread{1.2} % Adjust the line spacing here
\setlength{\headsep}{0.05in} % Adjust the space between header and text
\setlength{\footskip}{0.1in} % Adjust the space between text and footer
\parindent=0in
\flushbottom % Ensure text is aligned with both top and bottom margins

% \newcommand{\head}[3]{\lhead{#1}\chead{#2}\rhead{\ifthenelse{\isodd{
%     \thepage}}{Ishaan Sathaye
%  {\hspace{.01in}}}{}}}

% \head{Midterm 1 Exam Cheat Sheet}{STAT 334}

\begin{document}

% Halfway line for Midterm 1:
% \AddToShipoutPictureBG{
% \AtTextCenter{\hspace{-0.6\textwidth}\rule{1.2\textwidth}{0.5pt}}}

\textit{\textbf{\underline{1:}}}
\textbf{Scatterplot:} form/shape, direction (positive/negative), strength (
points follow recognizable form), unusual features (do not fit general trend).
\textbf{Association:} look at scales (might be misleading).
\textbf{PC Coeff:} strength of linear association; -1 to 1; 0 means no linear; r
does not change if variables are rescaled.
\textit{\textbf{\underline{2:}}}
\textbf{Residual:} observed - predicted; $e_i = y_i - \hat{y}_i$.
\textbf{Least Squares:} minimize SSE: $\sum_{i=1}^{n} e_i^2$.
\textbf{Coefficients:} determined by taking derivative of the SSE and solving
for the coeffs when setting to 0.
\textbf{Interp Coeff:} slope coeff as the pred change in Y associated with a one
-unit change in $X_i$, holding all other predictors constant; intercept as the 
predicted Y value when all preds are 0 (could be nonsensical).
\textbf{$R^2$:} coeff of determination; $ = \frac{SSE(\bar{y}) - SSE(\hat{y})}{SSE(\bar{y})}$;
unexplained variation in y / total var in y (look at points distribution).
\textbf{$R^2$ Interp:} $R^2 * 100$ is the percent reduction in SSE by taking into
account the predictors; percentage of variation in Y explained by the regression 
function with predictors; range is 0 to 1 and 1 if perfect predictions; $R^2 = r^2$
for simple linear regression.
\textit{\textbf{\underline{3:}}}
\textbf{Least Squares Estimate of $\beta$:} $SSE = \sum_{i=1}^{n} e^2_i = e^Te$;
$\hat{\beta} = (X^TX)^{-1}X^Ty$; $\hat{y} = X\hat{\beta} = Hy$.
\textbf{Hat Matrix:} $H = X(X^TX)^{-1}X^T$; symmetric and $h_{ij}$ describes 
the weight each of the values in the ith row of H have on the predicted value 
of $y_i$.
\textbf{Contributions:} $h_{ij}y_j$ is the actual contribution of j makes to the 
value $\hat{y_i}$.
\textit{\textbf{\underline{4:}}}
\textbf{MSE:} $s^2 = \frac{SSE}{n - p}$; $s$ or RMSE is the typical prediction 
error; expect 95\% of observed y values to lie roughly within $2s$ of predicted 
values; called \textbf{residual standard error in R}.
\textbf{s vs $R^2$:} both small s and large $R^2$ is the goal.
\textit{\textbf{\underline{5:}}}
\textbf{Perm. Test:} is there a relation between y and x find test stat that 
measures association for all possible perms of the resp var and compute the prop.
of times an observed test stat as extreme as the one from original sample; p-value 
from the graph by counting.
\textbf{p-value:} probability of obtaining a result (test stat) at least as extreme 
as the one observed, if the null were true; $<$ 0.1 is some, $<$ 0.05 is fairly 
strong, $<$ 0.01 is very strong, $<$ 0.001 is extremely strong; small p-value 
means result is unlikely to have occurred by change alone, if the null were true 
making it statistically significant.
\textbf{Inference on $\beta_j$:} $t = \frac{\hat{\beta}_j - \beta_j}{SE(\hat{\beta}_j)}$;
$ 1 - \alpha$ confidence limits: $\hat{\beta}_j \pm t_{n-p, \alpha/2}SE(\hat{\beta}_j)$.
\textbf{Standard Error of $\beta$:} $SE(\hat{\beta}_j) = \sqrt{MSE(C_{j+1}C_{j+1})}$ 
where C is the jth column of $(X^TX)^{-1}$; \texttt{summary(fit)} gives null = 0
and alternative $\neq$ 0; \texttt{model.matrix(fit)} gives design matrix X.
\textbf{One Sided H:} \texttt{pt()} with lower tail false gives area to the right
(t $>$ 0) and true gives left tail area.
\textbf{68/95/99.7 Rule:} values like within 1/2/3 std dev of mean.
\textbf{t-value:} t $>$ 2 or $<$ -2 means results are significant.
\textbf{Critical Value:} \texttt{qt(p, df)} gives t value for area p to the left of it;
90/95/99 percent CI is p = 0.95/0.975/0.995.
\textbf{CI Slope Interp:} We are 95\% confident that the expected \textit{change} 
in the \textit{response} for each one \textit{unit} increase in the \textit{$X_i$}
falls between l and u, (after adjusting for other predictors in the model) 
\textbf{use only when interpreting about the effect of one predictor with multiple
predictors, say for conclusions too in inference tests}.
\textbf{Bonferroni:} $1 - \frac{1-C}{g}$ where C is the confidence level and g is
the number of coefficients being tested; instead of $\alpha/2$ in each tail 
use $\frac{\alpha}{2g}$ in each tail; want each individual confidence to be 
above C in order for the joint level to be above C; \textbf{only for intervals}.
\textbf{Joint CI Interp:} We are (at least) 95\% confident that all intervals 
correctly capture the population parameters.
\underline{6:}
\textbf{FINE Assumptions:} Form, Independence, Normality, Equal Variance;
Form: expect linear form, Independence: errors are independent (in data 
description), Normality: errors follow a normal dist, Equal Variance: variance 
of errors is the same.
\textbf{Plots:} Form: residuals vs fitted (no trend/curve) or residuals vs each 
$X_i$ \textbf{for multiple predictors}; Equal Variance: residuals vs fitted 
(no fan shape); Normality: qq plot (no big departure from straight line) or 
histogram (no big skew); Independence: look at residuals vs observations 
number (want to be random).
\textbf{Formal tests:} wilks $\Rrightarrow$ normality, pagan $\Rrightarrow$ equal 
variance, low p means violated.
\underline{7:}
\textbf{Interp of Slope:} only log transforms can be restated in terms of the 
original vars, preds can always be restated in terms of original vars.
\textbf{Transforming} Y: non-linearity, non-constant variance, and non-normality; 
X: non-linearity, high leverage, influence; changing space = correct problems.
\textbf{Ladder of Powers:} p = 2, 1, 0.5, 0, -0.5, -1, -2; y* = $y^2, y,
\sqrt{y}, 1/\sqrt{y}, 1/y, 1/y^2$; right to become better; log and sqrt not
defined for zero or negative values, so transform $y/x + c$, where c makes all 
values $\geq 1$.
\textbf{Strategies:} skewed residuals: right skew is y down, left skew is y up;
residual var inc. as x incr: y down, decr as x incr: y up; non-linear: correct 
non-normal and unequal var then y, only non-linear then x.
\textbf{Non-linearity Bulges Point:} up and left is y up or x down; up and 
right is y up or x up; down and left is y down or x down; down and right is y 
down or x up.
\textbf{Interp of X Transform:} If we multiply $x_i$ by b (chosen log base) we 
predict a change of $\hat{\beta_i}$ in the mean value of y after adjusting for 
the other vars in model.
\textbf{Interp of Y Transform:} Each one unit change in $x_i$ changes the 
predicted median value of y by a factor of $b^{\hat{\beta_i}}$ after \dots.
\textbf{Interp of Both:} A c-fold change in $x_i$ changes the predicted median 
value of y by a factor of $c^{\hat{\beta_i}}$ after \dots.
\textbf{Non-log Transform:} Each increase of one in $1/x_i$ is associated with 
an increase of $\hat{\beta_i}$ in predicted $\sqrt{y}$.
\textbf{Median:} median instead of mean because $E[log(Y)] \neq log[E(Y)]$ but 
for median it is true.
\textbf{Box-Cox:} round lambda to nearest 0.5.
\textbf{Matrix Scatterplot:} don't reflect preds act jointly.
\\

\textit{\textbf{\underline{8A:}}}
\begin{itemize}
    \item R-sq equation: partitioning variability (sequential sum of squares)
    SSR, SSE, SSTO, which is R-sq
    \item Adding vars moves SSE to SSR
    \item Type 1: order matters, so far R-sq interp include after adjusting for
    var if after 1st var
    \item Type 1 SS block of info: anova(fit)
    \item percent of variation in y explained by adding var to model already 
    containing the vars \dots
    \item Type 2 SS: Anova(fit, type = 2); var entered into model last; 
    additional var in y explained by var after adjusting for all other vars;
    not sequential
\end{itemize}

\textit{\textbf{\underline{8B:}}}
\begin{itemize}
    \item degrees of freedom for total, error, reg model; SS divided by df is MS
    \item adding additional vars can never decrease R-sq
    \item R-adj-sq (no interp) equation
    \item General F test properties: p-values found and degrees of freedom
    \item Partial F-Test: anova(fit.reduced, fit.full) where reduced is the
    model without vars; null: r of betas is 0, alt: at least one beta is not 0; 
    F stat equation; explain equation; 
    are found
    \item Model Utility Test: summary(fit) where fit is full model; null: all 
    betas are 0, alt: at least one beta is not 0; F stat equation and explain 
    equation; reduced model is the mean model (no predictors)
    \item Single Coefficient Test: summary(fit) or Anova(fit, type='II') where 
    fit is full model; null: one beta is 0, alt: not 0; F stat equation; reduced 
    model is full model without 1 var; bold: equivalent to t-test t-sq = F
    \item Bold: Type 1 ANOVA table contains SST if you sum the SS for all;
    Type 2 ANOVA does not contain SST because it is not sequential
    \item Model Utility Test Decision: reject null: sufficient evidence...; 
    conclude that at least one of vars ... is significantly useful in 
    predicting y
    \item Single Coef Test Decision: large p-val means we do not have enough 
    evidence to to believe that var is any diff from zero after adj...., that 
    is var does not significantly improve the model containing other vars
    \item Bold: Cannot just drop all variables if not significant, since taking 
    them out would affect significance of other vars, need to come out one at a
    time or perform partial F test
    \item Partial F Test Decision: reject null: adding r vars to model that 
    already contains other vars does not significantly improve the model. Do 
    not have enough evidence that the full model containing all vars is better 
    than reduced model. So keep reduced model because of parsimony; larger model 
    achieves a significant reduction in SSE, boost in SSR and R-sq and improves 
    pred of y
    \item Partial F Null Hypoths: adding vars to smaller to form larger does not 
    significantly improve prediction of y, pop reg coefs for vars in larger not 
    in smaller model are 0, value of pop R-sq for larger model is not greater 
    than value of param for smaller model
\end{itemize}

\textit{\textbf{\underline{10:}}}


\textit{\textbf{\underline{11:}}}

\textit{\textbf{\underline{12:}}}

\textit{Midterm 2 Notes}
\begin{itemize}
    \item hard question is about $R^2$ because given a bunch of output and need 
    to know where to look; handout 8 anova tables
    \item 2 of the 3 F tests are on the Midterm
    \item Nothing about handout 12 really on the midterm
    \item No VIF problems but on that handout talked about added variable plots
    which is important
    \item transforming is back on the midterm (interpretations)
    \item more plots that we have seen before (harder plots) residuals vs fitted
    can have more than 1 assumption can be met
    \item stuff on the sequential sum of squres (handout 8) - focus on this
\end{itemize}

\textit{Midterm 2 Notes: Krish's Class}
\begin{itemize}
    \item lots of tables
    \item lots of stuff from Handout 8
    \item stuff on multicollinearity, nothing about cook's distance, not much on
    handout 12, nothing about leverage
    \item lots of questions from HW 4, on anova type 1 and type 2, tables, 
    sequential, etc.
    \item hard question about Anova tables
    \item know the SS for Anova and know true differnce between type 1 and type 
    2 and whento use each specifically
    \item one question from H10 about the intervals and R output (detailed 
    questions about the output), know interpretation of this too
    \item question on transformation similar to previous Midterm
    \item hard question on $R^2$ specifically
\end{itemize}

% \textit{Midterm Notes}
% \begin{itemize}
%     \item 12 open ended questions and 10 MC
%     \item issue with linearity with residuals vs fitted by looking at the form
%     \item 2 matrix problems (hat matrix)
%     \item 5 questions on the assumptions
%     \item transformation question is the hardest (worth 6 points)
%     \item after adjusting \dots when multiple predictors
%     \item given $R^2$ figure out s (typical prediction error) if hidden 
%     then find the numbers to calculate each thing
%     \item hard question on the bonferoni
%     \item know how to calculate a p-value (shade the area) (permutation test handout)
% \end{itemize}

\end{document}