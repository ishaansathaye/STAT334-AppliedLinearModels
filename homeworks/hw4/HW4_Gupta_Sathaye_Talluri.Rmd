---
title: "Homework 3"
subtitle: STAT 334
author: "Krishnanshu Gupta, Ishaan Sathaye, Sreshta Talluri"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: readable
    highlight: haddock
    toc: true
    toc_float: true
    code_folding: show
editor_options: 
  chunk_output_type: console
---

```{r chunk setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval=TRUE,
                      message=FALSE,
                      warning=FALSE)
```

```{r packages, include=FALSE}
library(readxl)
setwd( dirname( rstudioapi::getActiveDocumentContext( )$path ) )
```

# Problem 3

Suppose we are interested in predicting the selling price of the homes sold in Northampton, Massachusetts in 2014 (adjusted price from 2007).   The main response variable is price of the house in 2014 in thousands of dollars (would need to multiply by 1000 to get the actual price). The variables we will consider for this model: 

Y= price2014 \
X1= distance (distance to the bike trail (miles))\
X2= square+feet (square feet of the house)\
X3= no_rooms (number of ALL rooms in the house)\
X4= no_full_baths (number of Full baths)\
X5=  walkscore (Walk Score measures the walkability of any address using a patented system. Points are awarded based on the distance to amenities in each category. Amenities within a 5 minute walk provide a higher walkscore.) 

```{r}
rails = read_excel("./Rails.xlsx")
head(rails)
```

## Part (a)

```{r}
model1 = lm(price2014 ~ distance + squarefeet + no_rooms + no_full_baths + walkscore, data = rails)
summarym1 <- summary(model1)
summarym1
```

Sample Regression Equation: $\hat{Y} = 9.4664 + 2.9434X_1 + 116.1915X_2 + 2.5481X_3 + 29.2217X_4 + 1.0048X_5$, where $\hat{Y}$ is the predicted price of the house in 2014 in thousands of dollars and $X_1$, $X_2$, $X_3$, $X_4$, and $X_5$ are the distance, square feet, number of rooms, number of full baths, and walk score of the house respectively.

Population Regression Equation: $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \epsilon$, where $Y$ is the price of the house in 2014 in thousands of dollars, $X_1$, $X_2$, $X_3$, $X_4$, and $X_5$ are the distance, square feet, number of rooms, number of full baths, and walk score of the house respectively, $\beta_0, \beta_1, \beta_2, \beta_3, \beta_4, \beta_5$ are the population parameters, and $\epsilon$ is the error term.

## Part (b)

```{r}
summarym1$sigma
```

s = 62.507 means that the average difference between the observed values and the predicted values is 62.507. Also we expect 95% of the differences to be within 125.014 (2s) of the predicted value.

```{r}
summarym1$r.squared
```

0.698 means that 69.8% of the variability in the price of the house in 2014 can be explained by the distance, square feet, number of rooms, number of full baths, and walk score of the house.

## Part (c)

116.1915 is the predicted increase in the price of the house in 2014 in thousands of dollars for every additional square foot of the house, holding all other predictors constant. 1.0048 is the predicted increase in the price of the house in 2014 in thousands of dollars for every additional walk score of the house, holding all other predictors constant.

## Part (d)

```{r}
par(mfrow=c(2,4)) 
plot(resid(model1)~fitted(model1),ylab="Residuals",xlab="Fitted",pch=19)
abline(h=0,lty=2)

plot(resid(model1)~distance, data=rails,ylab="Residuals",pch=19)
abline(h=0,lty=2)

plot(resid(model1)~squarefeet,data=rails,ylab="Residuals",pch=19)
abline(h=0,lty=2)

plot(resid(model1)~no_rooms,data=rails,ylab="Residuals",pch=19)
abline(h=0,lty=2)

plot(resid(model1)~no_full_baths,data=rails,ylab="Residuals",pch=19)
abline(h=0,lty=2)

plot(resid(model1)~walkscore,data=rails,ylab="Residuals",pch=19)
abline(h=0,lty=2)

#normal Q-Q plot and histogram of residuals
qqnorm(resid(model1),ylab="Residuals"); qqline(resid(model1),lty=3)
hist(resid(model1),main="", xlab="Residuals",col='lightblue')
```

```{r}
library('lmtest')
shapiro.test(resid(model1))
bptest(model1)
```

From the residual analysis, the assumption for linearity has not been violated since the qq plot has no deviations from the line, although there is a presence of an outlier. There does seem to be a violation for the assumption of equal variance since when taking a look at the residuals vs distance plot there is a fan shape. That patter can also be seen in the no_rooms vs residuals plot. This is further reinforced by the Breusch-Pagan test which resulted in a p value \< 0.001, which means there is sufficient evidence to reject the null that assumption of equal variance has been violated. For the assumption of normality, the Shapiro-Wilk test that was conducted resulted in significant results, which means the assumptions has been violated however this could be due to the outlier. Taking a look at the distribution of the residuals vs frequency histogram shows a plot which would be normal without the outlier.

## Part (e)

**Model-Utility Test**

$H_0$: $\beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$, where $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$ are the population parameters for distance, square feet, number of rooms, number of full baths, and walk score of the house respectively.

$H_a$: At least one of $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$, where $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$ are the population parameters for distance, square feet, number of rooms, number of full baths, and walk score of the house respectively, is not equal to 0.

```{r}
summarym1
```

Test Statistic: F-statistic is 45.23

Degrees of Freedom: k = 5 and n - k - 1 = 98

p-value: 2.2e-16

Decision: Reject Null Hypothesis

Conclusion: We have strong evidence to conclude at least one of the $\beta_i$'s (i = 1,...,5), where $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$ are the population parameters for distance, square feet, number of rooms, number of full baths, and walk score of the house respectively, is not equal to 0. We can conclude that at least one of the variables (distance, square feet, number of rooms, number of full baths, and walk score) is a significantly useful in predicting the price of the house in 2014.

## Part (f)

**Single-Coefficient Test on X1 (distance)**

$H_0$: $\beta_1 = 0$, where $\beta_1$ is the population parameter for distance.

$H_a$: $\beta_1 \neq 0$, where $\beta_1$ is the population parameter for distance.

```{r}
summarym1 # or Anova(model1, type='II')
```

Test Statistic: F-statistic is $t^2$ = 0.080089

Degrees of Freedom: k = 1 and n - k - 1 = 98

p-value: 0.7781

Decision: Fail to Reject Null Hypothesis

Conclusion: Large p-value implies we do not have enough evidence to believe that $\beta_1$, which is distance, is any different from zero after adjusting for the other variables (square feet, number of rooms, number of full baths, and walk score of the house). Distance does not significantly improve the model containing the other variables.

## Part (g)

**Single-Coefficient Test on X2 (squarefeet)**

$H_0$: $\beta_2 = 0$, where $\beta_2$ is the population parameter for square feet.

$H_a$: $\beta_2 \neq 0$, where $\beta_2$ is the population parameter for square feet.

```{r}
summarym1 # or Anova(model1, type='II')
```

Test Statistic: F-statistic is $t^2$ = 24.97001

Degrees of Freedom: k = 1 and n - k - 1 = 98

p-value: 2.54e-06

Decision: Reject Null Hypothesis

Conclusion: Small p-value implies we have strong evidence to believe that $\beta_2$, which is square feet, is different from zero after adjusting for the other variables (distance, number of rooms, number of full baths, and walk score of the house). Square feet is a significant predictor of the model containing the other variables.

## Part (h)

**Partial F-Test on X1 and X3, (distance and no_rooms)**

$H_0$: $\beta_1 = \beta_3 = 0$, where $\beta_1$ and $\beta_3$ are the population parameters for distance and no_rooms.

$H_a$: At least one of $\beta_1$ and $\beta_3 \neq 0$, where $\beta_1$ and $\beta_3$ are the population parameters for distance and no_rooms.

```{r}
library(car)

model2 <- lm(price2014 ~ squarefeet + no_full_baths + walkscore, data = rails)
anova(model2, model1)
```

Test Statistic: F-statistic is 0.1208

Degrees of Freedom: r = 2 and n - k - 1 = 98

p-value: 0.8863

Decision: Fail to Reject Null Hypothesis

Conclusion: We fail to reject the null hypothesis (large p value). Therefore adding these 2 variables, distance and no_rooms, to a model already containing the other 3 variables (square feet, number of full baths, and walk score) does not significantly improve the model. We do not have enough evidence that the full model containing all variables is significantly better than the model with just 3 variables. So we can keep with model 2 because of parsimony.

## Part (i)

**Partial F-Test on X3 and X4, (no_rooms and no_full_baths)**

$H_0$: $\beta_3 = \beta_4 = 0$, where $\beta_3$ and $\beta_4$ are the population parameters for no_rooms and no_full_baths.

$H_a$: At least one of $\beta_3$ and $\beta_4 \neq 0$, where $\beta_3$ and $\beta_4$ are the population parameters for no_rooms and no_full_baths.

```{r}
library(car)

model3 <- lm(price2014 ~ distance + squarefeet + walkscore, data = rails)
anova(model3, model1)
```

Test Statistic: F-statistic is 3.0247

Degrees of Freedom: r = 2 and n - k - 1 = 98

p-value: 0.05313

Decision: Reject Null Hypothesis

Conclusion: We reject the null hypothesis since p \< 0.1. Therefore adding these 2 variables, no_rooms and no_full_baths, to a model already containing the other 3 variables (distance, square feet, and walk score) does significantly improve the model. We have some evidence that the larger model achieves a significant reduction in SSE and improves the prediction of the price. It also means that one of the population regression coefficients of the variables in the larger model that are not in the smaller model (model 3) are nonzero.

## Part (j)

Model 2 is better than model 1 due to parsimony, which means that the model with fewer parameters is favored over a complex model. I decided this from the p-value from the partial f test, where I failed to reject the null hypothesis that adding variables to the smaller model to form the larger model does not significantly improve the prediction of Y.

## Part (k)

Model 3 is better than model 2 due to the marginal improvement in prediction by including the variable number of full baths. I decided this because in the second partial f test, I really tested whether including the variable no_full_baths significantly improves the model fit compared to model 1, which includes all the variables. Since the p-value is less than 0.1, it indicates a marginal improvement in fit by including no_full_baths. So considering this marginal significance and the principle of parsimony, model 3 is better than model 2.

## Part (l)

```{r}
confint(model1, level = (1 - 0.05/5))
```

We are at least 95% confident that all intervals correctly capture the true population parameters.
